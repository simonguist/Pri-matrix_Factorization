{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Licensed to the Apache Software Foundation (ASF) under one\n",
    "or more contributor license agreements.  See the NOTICE file\n",
    "distributed with this work for additional information\n",
    "regarding copyright ownership.  The ASF licenses this file\n",
    "to you under the Apache License, Version 2.0 (the\n",
    "\"License\"); you may not use this file except in compliance\n",
    "with the License.  You may obtain a copy of the License at\n",
    "\n",
    "  http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing,\n",
    "software distributed under the License is distributed on an\n",
    "\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "KIND, either express or implied.  See the License for the\n",
    "specific language governing permissions and limitations\n",
    "under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## train.ipynb\n",
    "- Training and evaluating for the Chimp&See dataset (DrivenData Pri-matrix Factoriation competition, 2017)\n",
    "- All layers until the convolutional layers are initialized from the RGB-I3D model (Carreira and Zimmerman, 2017) that was trained on the Kinetics dataset and pretrained on Imagenet\n",
    "- Code was designed in a way that extension to multi-gpu training should be possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "#source: https://github.com/deepmind/kinetics-i3d\n",
    "from kinetics_i3d import i3d\n",
    "#modified from https://gist.github.com/drivendata/70638e8a9e6a10fa020623f143259df3\n",
    "import primatrix_dataset_utils\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sonnet as snt\n",
    "import threading\n",
    "import os\n",
    "import time\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "WEIGHT_DECAY = 1e-7\n",
    "LEARNING_RATE = 1e-1\n",
    "DROPOUT_KEEP_PROB = 0.5\n",
    "BATCH_SIZE = 6\n",
    "VAL_SIZE = 0.3\n",
    "\n",
    "FREQUENCIES_OVERSAMPLING = True\n",
    "ANIMAL_OVERSAMPLING = False\n",
    "OVERSAMPLING_FREQUENCIES = [ 0.50082119,  0.21659901,  0.63500282,  0.44675957,  0.5577173 ,\n",
    "  0.90370611,  0.68986291,  0.68944916,  0.34719383,  0.89609911,\n",
    "  0.67162594,  0.67663023,  1.0123    ,  0.51791462,  0.34569939,\n",
    "  0.76321204,  0.60431893,  0.91221001,  0.48646224,  0.65734299,\n",
    "  0.74687231,  0.84253148,  0.34175657,  0.45264604]\n",
    "\n",
    "TRAIN_STEPS = 10\n",
    "VAL_STEPS = 0\n",
    "CREATE_PREDICTIONS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "IMAGE_SIZE = 224\n",
    "NUM_CLASSES = 24\n",
    "SAMPLE_VIDEO_FRAMES = 90\n",
    "N_TEST_IMAGES = 87485"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "CHECKPOINT_PATH_KINETICS_IMAGENET_RGB = 'kinetics_i3d/rgb_imagenet/model.ckpt'\n",
    "TENSORBOARD_PATH = 'training_results'\n",
    "MODEL_LOAD_PATH = 'models/model_final.ckpt'\n",
    "MODEL_SAVE_PATH = 'models/model_final.ckpt'\n",
    "DATASET_PATH = 'data/'\n",
    "TEST_SET_PREDICTIONS_FILE = 'predictions_test.csv'\n",
    "\n",
    "# make sure tensorboard path exists\n",
    "if not os.path.exists(TENSORBOARD_PATH):\n",
    "    os.makedirs(TENSORBOARD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# build the model: take convolutional layers from i3d rgb model and create fully connected layers\n",
    "def get_logits(inputs, is_training, dropout_keep_prob):\n",
    "    with tf.variable_scope('RGB'):\n",
    "        model_i3d = i3d.InceptionI3d(\n",
    "          NUM_CLASSES, spatial_squeeze=True, final_endpoint='Mixed_5c')\n",
    "        mixed_5c, _ = model_i3d(\n",
    "          inputs, is_training=is_training, dropout_keep_prob=dropout_keep_prob)\n",
    "    with tf.variable_scope('Logits'):\n",
    "        net = tf.nn.avg_pool3d(mixed_5c, ksize=[1, 2, 7, 7, 1],\n",
    "                                 strides=[1, 1, 1, 1, 1], padding=snt.VALID)\n",
    "        net = tf.nn.dropout(net, dropout_keep_prob)\n",
    "        logits = i3d.Unit3D(output_channels=NUM_CLASSES,\n",
    "                          kernel_shape=[1, 1, 1],\n",
    "                          activation_fn=None,\n",
    "                          use_batch_norm=False,\n",
    "                          use_bias=True,\n",
    "                          name='Conv3d_0c_1x1')(net, is_training=is_training)\n",
    "        logits = tf.squeeze(logits, [2, 3], name='SpatialSqueeze')\n",
    "        logits = tf.reduce_mean(logits, axis=1)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# average gradients (would be necessary for multi gpu training)\n",
    "def average_gradients(tower_grads):\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "    # Note that each grad_and_vars looks like the following:\n",
    "    # ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "            grads.append(expanded_g)\n",
    "        grads_concat = tf.concat(grads, axis=0)\n",
    "        grads_mean = tf.reduce_mean(grads_concat, axis=0)\n",
    "        v = grad_and_vars[0][1]\n",
    "        average_grads.append((grads_mean, v))\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# restore the pretrained weights, except for the last layer\n",
    "def restore():\n",
    "    rgb_variable_map = {}\n",
    "    for variable in tf.global_variables():\n",
    "        if variable.name.split('/')[0] == 'RGB':\n",
    "            if 'Logits' in variable.name or 'batch_norm' in variable.name or 'Momentum' in variable.name:\n",
    "                continue\n",
    "            rgb_variable_map[variable.name.replace(':0', '')] = variable\n",
    "    saver = tf.train.Saver(var_list=rgb_variable_map, reshape=True)\n",
    "    return saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def inference(inputs, labels, is_training, dropout_keep_prob):\n",
    "    logits = get_logits(inputs, is_training, dropout_keep_prob)\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=logits, labels=labels))\n",
    "    varsList = tf.trainable_variables()    \n",
    "    loss_L2 = tf.add_n([tf.nn.l2_loss(v) for v in varsList\n",
    "                                    if 'bias' not in v.name]) * WEIGHT_DECAY\n",
    "    return  loss, loss + loss_L2, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#load and preprocess videos for training/testing\n",
    "def load_and_enqueue(sess,\n",
    "                     dequeue_data_indices_op,\n",
    "                     enqueue_op,\n",
    "                     data_input,\n",
    "                     labels_input,\n",
    "                     data_filenames_input,\n",
    "                     train_steps,\n",
    "                     val_steps,\n",
    "                     test_steps):\n",
    "    feed_dict = {}\n",
    "    for _ in range(train_steps):\n",
    "        next_indices = sess.run(dequeue_data_indices_op)\n",
    "        batch = data.batches_by_indices(next_indices)\n",
    "        feed_dict[data_input] = batch[0]\n",
    "        feed_dict[labels_input] = batch[1]\n",
    "        feed_dict[data_filenames_input] = np.empty([BATCH_SIZE], dtype = str)\n",
    "        sess.run(enqueue_op, feed_dict=feed_dict)\n",
    "    for _ in range(val_steps):\n",
    "        next_indices = sess.run(dequeue_data_indices_op)\n",
    "        batch = data.val_batches_by_indices(next_indices)\n",
    "        feed_dict[data_input] = batch[0]\n",
    "        feed_dict[labels_input] = batch[1]\n",
    "        feed_dict[data_filenames_input] = np.empty([BATCH_SIZE], dtype = str)\n",
    "        sess.run(enqueue_op, feed_dict=feed_dict)\n",
    "    for _ in range(test_steps):\n",
    "        next_indices = sess.run(dequeue_data_indices_op)\n",
    "        batch = data.test_batches_by_indices(next_indices)\n",
    "        feed_dict[data_input] = batch\n",
    "        feed_dict[labels_input] = np.ones([BATCH_SIZE, NUM_CLASSES])*np.nan\n",
    "        feed_dict[data_filenames_input] = np.array(data.X_test_ids[next_indices].values)\n",
    "        sess.run(enqueue_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#load indices of videos for training/testing\n",
    "def enqueue_indices(indices_generator_train, indices_generator_val, indices_generator_test, \\\n",
    "                    sess, indices_loader_worker_ops, \\\n",
    "                    train_steps, val_steps, test_steps):\n",
    "    feed_dict = {}\n",
    "    for _ in range(train_steps):\n",
    "        for data_indices, enqueue_indices_op in indices_loader_worker_ops:\n",
    "            batch = next(indices_generator_train)\n",
    "            feed_dict[data_indices] = batch\n",
    "            sess.run(enqueue_indices_op, feed_dict=feed_dict)\n",
    "    for _ in range(val_steps):\n",
    "        for data_indices, enqueue_indices_op in indices_loader_worker_ops:\n",
    "            batch = next(indices_generator_val)\n",
    "            feed_dict[data_indices] = batch\n",
    "            sess.run(enqueue_indices_op, feed_dict=feed_dict)\n",
    "    for _ in range(test_steps):\n",
    "        for data_indices, enqueue_indices_op in indices_loader_worker_ops:\n",
    "            batch = next(indices_generator_test)\n",
    "            feed_dict[data_indices] = batch\n",
    "            sess.run(enqueue_indices_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = primatrix_dataset_utils.Dataset(datapath=DATASET_PATH,\n",
    "               dataset_type = 'small',\n",
    "               reduce_frames=True, \n",
    "               batch_size=BATCH_SIZE, \n",
    "               test=False, \n",
    "               val_size = VAL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring from: ../aws_nvirginia/efs/models/model_tb_8GPU_oversampling01_no_2.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ../aws_nvirginia/efs/models/model_tb_8GPU_oversampling01_no_2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 10/10 [01:46<00:00, 10.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:total_time: 122s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "time_start = time.clock()\n",
    "\n",
    "tower_grads = []               #gradients\n",
    "tower_scores = []              #scores (loss without L2 regularization)\n",
    "tower_losses = []              #loss + L2 regularization\n",
    "tower_inference_test = []      #indices and sigmoid probabilities for inference on test set \n",
    "\n",
    "data_loader_worker_ops = []    #ops that are required by the data loading process\n",
    "indices_loader_worker_ops = [] #ops that are required by the indices loading process\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=False,\n",
    "                                      log_device_placement=False,\n",
    "                                      intra_op_parallelism_threads=24)) as sess:\n",
    "\n",
    "    with tf.variable_scope(tf.get_variable_scope()):\n",
    "        with tf.name_scope('tower'):\n",
    "            with tf.device('/cpu:0'):\n",
    "                is_training = tf.placeholder(tf.bool)\n",
    "                dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "                opt = tf.train.MomentumOptimizer(\n",
    "                learning_rate=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "                data_indices = tf.placeholder(\n",
    "                        tf.int32, shape = (BATCH_SIZE))\n",
    "                queue_data_indices = tf.FIFOQueue(capacity=300, dtypes=[tf.int32], \\\n",
    "                                 shapes=[[BATCH_SIZE]])\n",
    "                enqueue_data_indices_op = queue_data_indices.enqueue([data_indices])\n",
    "                dequeue_data_indices_op = queue_data_indices.dequeue()\n",
    "                labels_input = tf.placeholder(tf.float32, shape=(BATCH_SIZE, NUM_CLASSES))\n",
    "                data_filenames_input = tf.placeholder(tf.string, shape =(BATCH_SIZE))\n",
    "                data_input = tf.placeholder(\n",
    "                        tf.float32,\n",
    "                        shape=(BATCH_SIZE, SAMPLE_VIDEO_FRAMES, IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "                queue = tf.FIFOQueue(capacity=5, dtypes=[tf.float32, tf.float32, tf.string],\n",
    "                                 shapes=[[BATCH_SIZE, SAMPLE_VIDEO_FRAMES, IMAGE_SIZE, IMAGE_SIZE, 3],\n",
    "                                         [BATCH_SIZE, NUM_CLASSES],\n",
    "                                         [BATCH_SIZE]] )\n",
    "                enqueue_op = queue.enqueue([data_input, labels_input, data_filenames_input])\n",
    "                data_loader_worker_ops.append([data_input,\n",
    "                                               labels_input,\n",
    "                                               data_filenames_input,\n",
    "                                               dequeue_data_indices_op,\n",
    "                                               enqueue_op])\n",
    "                indices_loader_worker_ops.append([data_indices, enqueue_data_indices_op])\n",
    "\n",
    "            with tf.device('/gpu:0'):\n",
    "                [data_input, labels, data_labels] = queue.dequeue()\n",
    "                score, loss, logits = inference(data_input, labels, is_training, dropout_keep_prob)\n",
    "                grads = opt.compute_gradients(loss)\n",
    "                tower_grads.append(grads)\n",
    "                tower_losses.append(loss)\n",
    "                tower_scores.append(score)\n",
    "\n",
    "            with tf.device('/cpu:0'):\n",
    "                sigmoid_probabilities = tf.nn.sigmoid(logits)\n",
    "                tower_inference_test.append([data_labels, sigmoid_probabilities])\n",
    "                    \n",
    "    with tf.device('/cpu:0'):\n",
    "        global_step_tensor = tf.Variable(0, trainable=False, name='global_step')\n",
    "        avg_loss = tf.reduce_mean(tower_losses)\n",
    "        avg_score = tf.reduce_mean(tower_scores)\n",
    "        grads = average_gradients(tower_grads)\n",
    "        train_op = opt.apply_gradients(grads, global_step_tensor)\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        if os.path.exists(MODEL_LOAD_PATH + \".index\"):\n",
    "            #Restore from existing model\n",
    "            tf.logging.info('Restoring from: %s', MODEL_LOAD_PATH)\n",
    "            saver.restore(sess, MODEL_LOAD_PATH)\n",
    "        else:\n",
    "            #Create new model with pretrained weights from i3d model trained on kinetics dataset\n",
    "            pretrained_saver = restore()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            tf.logging.info('No checkpoint file found, restoring pretrained weights...')\n",
    "            pretrained_saver.restore(sess, CHECKPOINT_PATH_KINETICS_IMAGENET_RGB)\n",
    "            tf.logging.info('Restore Complete.')\n",
    "\n",
    "        summary_writer_train = tf.summary.FileWriter(TENSORBOARD_PATH + \"/plot_train\", sess.graph)\n",
    "        summary_writer_val = tf.summary.FileWriter(TENSORBOARD_PATH + \"/plot_val\", sess.graph)\n",
    "        tf.logging.set_verbosity(tf.logging.INFO) \n",
    "        \n",
    "        \n",
    "        def train(train_steps, val_steps=0, save_model=True, create_predictions=False):\n",
    "        \n",
    "            if create_predictions:\n",
    "                test_steps = N_TEST_IMAGES // (BATCH_SIZE) + 1\n",
    "            else:\n",
    "                test_steps = 0\n",
    "        \n",
    "            t = threading.Thread(target=enqueue_indices,\n",
    "                            args=(data.batches_with_oversampling_get_indices(OVERSAMPLING_FREQUENCIES),\n",
    "                                  data.val_batches_get_indices(),\n",
    "                                  data.test_batches_get_indices(),\n",
    "                                  sess, indices_loader_worker_ops,\n",
    "                                  train_steps,\n",
    "                                  val_steps,\n",
    "                                  test_steps,),\n",
    "                            daemon = True)\n",
    "            t.start()\n",
    "\n",
    "            for (data_input,\n",
    "                 labels_input,\n",
    "                 data_filenames_input,\n",
    "                 dequeue_data_indices_op,\n",
    "                 enqueue_op) in data_loader_worker_ops:\n",
    "                \n",
    "                t = threading.Thread(target=load_and_enqueue,\n",
    "                        args=(sess,\n",
    "                              dequeue_data_indices_op,\n",
    "                              enqueue_op, data_input,\n",
    "                              labels_input, data_filenames_input,\n",
    "                              train_steps,\n",
    "                              val_steps,\n",
    "                              test_steps,),\n",
    "                        daemon = True)\n",
    "                t.start()\n",
    "\n",
    "            if train_steps!=0:\n",
    "                score_train_sum = 0\n",
    "                for _ in tqdm.tqdm(range(train_steps),\n",
    "                                         total=train_steps,\n",
    "                                         desc = \"train\"):\n",
    "                    _, n_steps, loss_train, score_train = sess.run([train_op,\n",
    "                                                                    global_step_tensor,\n",
    "                                                                    avg_loss,\n",
    "                                                                    avg_score],\n",
    "                                                                    {is_training: True, dropout_keep_prob: 0.5})\n",
    "                    summary = tf.Summary(value=[tf.Summary.Value(\n",
    "                            tag=\"loss\", simple_value=loss_train)])\n",
    "                    summary_writer_train.add_summary(summary, n_steps)\n",
    "                    score_train_sum += score_train\n",
    "                score_train_avg = score_train_sum/train_steps\n",
    "                summary = tf.Summary(value=[tf.Summary.Value(\n",
    "                        tag=\"score\", simple_value=score_train_avg)])\n",
    "                summary_writer_train.add_summary(summary, n_steps)\n",
    "\n",
    "\n",
    "            if val_steps!=0:\n",
    "                if train_steps==0:\n",
    "                    n_steps = sess.run(global_step_tensor)\n",
    "                score_val_sum = 0\n",
    "                for _ in tqdm.tqdm(range(val_steps), total=val_steps, desc = \"val\"):\n",
    "                    score_val = sess.run([avg_score],\n",
    "                                         {is_training: True, dropout_keep_prob: 1.0})\n",
    "                    score_val = score_val[0]\n",
    "                    score_val_sum += score_val\n",
    "                score_val_avg = score_val_sum/val_steps\n",
    "                summary = tf.Summary(value=[tf.Summary.Value(\n",
    "                        tag=\"score\", simple_value=score_val_avg)])\n",
    "                summary_writer_val.add_summary(summary, n_steps)\n",
    "\n",
    "            if create_predictions:\n",
    "                for _ in tqdm.tqdm(range(test_steps),\n",
    "                                         total=test_steps,\n",
    "                                         desc = \"test\"):\n",
    "                    tower_inference_test_out = sess.run([tower_inference_test],\n",
    "                                                        {is_training: True, dropout_keep_prob: 1.0})\n",
    "                    tower_inference_test_out = tower_inference_test_out[0]\n",
    "                    for indices, probabilities in tower_inference_test_out:\n",
    "                        indices = [i.decode('utf-8') for i in indices]\n",
    "                        data.update_predictions_at(indices, probabilities)\n",
    "                    \n",
    "                #save predictions to file\n",
    "                data.predictions.to_csv(TEST_SET_PREDICTIONS_FILE)\n",
    "                \n",
    "\n",
    "            if save_model:\n",
    "                saver.save(sess, MODEL_SAVE_PATH)\n",
    "                tf.logging.info('Model saved as ' + MODEL_SAVE_PATH)\n",
    "            \n",
    "        train(train_steps = TRAIN_STEPS,\n",
    "              val_steps = VAL_STEPS,\n",
    "              save_model=False,\n",
    "              create_predictions=CREATE_PREDICTIONS)\n",
    "\n",
    "        summary_writer_train.close()\n",
    "        summary_writer_val.close()\n",
    "        \n",
    "time_stop = time.clock()\n",
    "tf.logging.info(\"total_time: %ds\", time_stop-time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
